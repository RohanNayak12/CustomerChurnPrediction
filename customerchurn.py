# -*- coding: utf-8 -*-
"""CustomerChurn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XhYakYc0qT887w79f8uLndeby2ptJKrp
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
rjmanoj_credit_card_customer_churn_prediction_path = kagglehub.dataset_download('rjmanoj/credit-card-customer-churn-prediction')

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk(rjmanoj_credit_card_customer_churn_prediction_path):
  for filename in filenames:
    print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

df=pd.read_csv('/root/.cache/kagglehub/datasets/rjmanoj/credit-card-customer-churn-prediction/versions/1/Churn_Modelling.csv')
df.head(7)

df.info()
print(df.duplicated().sum())

df['Exited'].value_counts()
df.drop(columns=['RowNumber','CustomerId','Surname'],inplace=True)

df=pd.get_dummies(df,columns=['Geography','Gender'],drop_first=True)

x=df.drop(columns=['Exited'])
y=df['Exited']
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=1)

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
x_train_scaled=sc.fit_transform(x_train)
x_test_scaled=sc.fit_transform(x_test)
print(x_train_scaled)

import tensorflow
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

from tensorflow.keras import regularizers
model=Sequential()
model.add(Dense(10,activation='relu',input_dim=11,kernel_regularizer=regularizers.l2(0.01)))
model.add(Dense(5,activation='relu',input_dim=11,kernel_regularizer=regularizers.l2(0.01)))
model.add(Dense(1,activation='sigmoid'))
model.summary()

model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])
trained_model=model.fit(x_train_scaled,y_train,epochs=100,validation_split=0.2)

model.layers[1].get_weights()

y_log=model.predict(x_test_scaled)

y_pred=np.where(y_log>0.5,1,0)

from sklearn.metrics import accuracy_score
accuracy_score(y_test,y_pred)

import matplotlib.pyplot as plt
trained_model.history

plt.plot(trained_model.history['loss'],label='train loss')
plt.plot(trained_model.history['val_loss'],label='val loss')
plt.legend()
plt.show()

plt.plot(trained_model.history['accuracy'],label='accuracy')
plt.plot(trained_model.history['val_accuracy'],label='val accuracy')
plt.legend()
plt.show()

